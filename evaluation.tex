\section{Model Comparison on the Hypersphere}
\label{sec:evaluation}
It is not immediately obvious what criteria to use to judge these models and decide which best
  represents the data's generating distribution.  As the task resides on $\mathcal{S}_{\infty}^{d-1}$,
  our options are somewhat limited in terms of what model criteria we can use.  Because of the aforementioned
  difficulty in evaluating a density directly in this space, we choose to use model evaluation
  criteria that can operate on samples from the posterior predictive distribution.  For this reason,
  we have opted to use the \emph{posterior predictive loss} criterion of \cite{gelfand1998} and the
  \emph{energy score} criterion of \cite{gneiting2007}.  Both of these metrics require calculating
  some distance in the target space, and this section will be devoted to that end.  As both these
  metrics operate on the posterior predictive distribution for each observation, we will also be
  including a metric operating on the overall posterior predictive distribution, in the form of a
  modified Kullbeck-Liebler divergence.

\subsection{Posterior Predictive Loss Criterion}
The posterior predictive loss criterion, \emph{PPL} is introduced in \cite{gelfand1998}.  When we
  assume a squared error loss function, then for the $i$th observation, the posterior predictive
  loss criterion is computed as
  \begin{equation}
    \label{eq:ppl}
    D_{k}^{(i)} = \text{Var}\left(X_i\right) + \frac{k}{k + 1}\left(\text{E}[X_i] - x_i\right)^2,
  \end{equation}
  where $X_i$ is a random variable from the posterior predictive distribution for $x_i$.  The
  scalar $k$ is a weighting factor by which we arbitrarily scale the importance of goodness of fit
  relative to precision.  In our analysis, we take the limit as $k\to\infty$, weighting both
  parts equally.  Interpreting this criterion, a smaller $\text{Var}(\bm{X}_{\ell})$ indicates a higher
  precision, and a smaller $(\text{E}[\bm{X}_{\ell}] - \bm{x}_{\ell})^2$  indicates a better fit.  Thus,
  smaller is better.  Note that this is defined for a univariate distribution.  As we are dealing
  with a multivariate distribution, we will be taking the posterior predictive loss
  summed over all dimensions.  That is,
  \begin{equation}
    \label{eq:ppl2}
    D_k^{(i)} = \frac{1}{d}\sum_{\ell = 1}^{d}\left[\text{Var}(X_{i,\ell}) +
                  \frac{k}{k+1}\left(\text{E} [X_{i,\ell}] - x_{i,\ell}\right)^2\right]
  \end{equation}
  Then we report the average $D_k$, taken over all $i$.

\subsection{Energy Score}
The energy score of Gneiting, et al\cite{gneiting2007}, is a generalization of the continuous ranked
  probability score, or \emph{crps}, defined for a multi-dimensional random variable.
  \begin{equation}
    \label{eq:es}
    \text{ES}\left(P, \bm{x}_i\right) =  \text{E}_p g\left(\bm{X}_i, \bm{x}_i\right) -
                                    \frac{1}{2}\text{E}_p g\left(\bm{X}_i,\bm{X}_i^{\prime}\right)
  \end{equation}
  where $g(\cdot)$ identifies a negative definite kernel, and $\bm{X}_i^{\prime}$ represents another
  replicate from the posterior predictive distribution of $\bm{x}_i$. A function $g$ is a negative definite kernel if it is symmetric in its arguments, $g(x_1,x_2) = g(x_2,x_1)$, and for which
  $\sum_{i =1}^n\sum_{j=1}^n\alpha_i\alpha_jg(\bm{x}_1,\bm{x}_2) \leq 0$ for all positive integers n, with the
  restriction that $\sum_{i=1}^n\alpha_i = 0$.  Euclidean distance is one example of a negative definite kernel,
  and the one most often used.  However, on $\mathcal{S}_p^{d-1}$ for $p > 1$, Euclidean distance
  can under-report the actual distance required for travel between two points. That distortion is
  maximized on $\mathcal{S}_{\infty}^{d-1}$.

\subsection{Distance on the positive orthant of the Hypercube}
  \label{subsec:distance}
The positive orthant of the unit hypercube, defined in Euclidean geometry, is that structure for
  which, in a given point on the hypercube, all dimensions of that point are between 0 and 1, and
  at least one dimension must be 1.  Developing terminology, we can consider observations for which
  the $j$th dimension is equal to 1, to be on the $j$th \emph{face}.  The intersection of the $i$th
  and $j$th face is a $d-2$ dimensional cube, and observations in this space have dimensions $i,j$
  equal to 1.

A \emph{distance} in this space is a \emph{geodesic} on this space. From geometry, we know that
  the geodesic, or shortest path between 2 points along the surface of a $d$ dimensional figure
  corresponds to at least one \emph{unfolding}, or \emph{rotation} of the $d$-dimensional
  figure into a $d-1$ dimensional space.  The appropriate term for the structure generated by this
  unfolding is a \emph{net}.  For the appropriate net, a line segment connecting the two points and
  staying within the boundaries of the net corresponds to the shortest path between those points\citep{pappas1989}, and is thus a geodesic.  The length of that line segment is properly
  defines the distance required for travel between those points.

Consider a 3-dimensional cube.  Consider 2 points on this 3-dimensional cube,
  $\bm{a}_1 = (x_1,y_1,z_1)$, and $\bm{a}_2 = (x_2,y_2,z_2)$. Let's say that the two points are
  on the same face.  Then the distance between those two points, the distance one has to travel
  along the space to move from one point to the other, is calculated by Euclidean norm.  Now,
  consider two points on separate faces.  All faces are pairwise adjacent, as we have stated, so
  in order to move to the other point, we must \emph{at least} move to the intersection between the
  faces, then to the other point.  Let $\bm{a}_1$ lie on the $x$ face, and $\bm{a}_2$ lie on the
  $y$ face.  That is, $\bm{a}_1 = (1, y_1, z_1)$, $\bm{a}_2 = (x_2, 1, z_2)$.  Then traveling
  between these points we must at least pass through the intersection of faces $x,y$.  One possible
  net representation of this is unfolding the $y$ face alongside the $x$ face.  We accomplish this
  by applying a rotation and translation to $a_2$, corresponding to the following:
  \begin{equation}
    \label{eq:1drotation}
    a_2^{\prime} = \begin{bmatrix}
    1 \\
    2 \\
    0
    \end{bmatrix}
    +
    \begin{bmatrix}
    0  & 0 & 0 \\
    -1 & 0 & 0 \\
    0  & 0 & 1
    \end{bmatrix}
    \begin{bmatrix}
    x_2 \\
    1 \\
    z_2 \\
    \end{bmatrix} = \begin{bmatrix}
    1 \\
    2 - x_2 \\
    z_2
    \end{bmatrix}
  \end{equation}
  Then, if this is the appropriate net, the distance from $\bm{a}$ to $\bm{b}$ becomes
  $\lVert \bm{a}_1 - \bm{a}_2^{\prime}\rVert_2$. However, there
  is another possible net we must consider, travelling first through the $z$ face then to the
  $y$ face.  The rotation for that becomes:
  \begin{equation}
    \label{eq:2drotation}
    a_2^{\prime} = \begin{bmatrix}
    1 \\
    2 \\
    2
    \end{bmatrix}
    +
    \begin{bmatrix}
    0 & 0 & 0  \\
    0 & 0 & -1 \\
    -1 & 0 & 0
    \end{bmatrix}
    \begin{bmatrix}
    x_2 \\
    1 \\
    z_2
    \end{bmatrix} = \begin{bmatrix}
    1 \\
    2 - z_2 \\
    2 - x_2
    \end{bmatrix}
  \end{equation}
Every successive rotation is relative to the last face.  So, as the number of dimensions grows, the
  number of possible rotations grows as well.  As we have $d$ faces, if 2 observations are on
  different faces, then there are $\sum_{j = 1}^{d-2}\binom{d-2}{j} + 1$ possible rotations to
  consider\footnote{There are truly $d!$ possible nets, but when we consider starting and ending
  faces fixed, and that portions of the net that diverge after the ending face are irrelevant,
  we arrive at that number of rotations that we need consider}.  While this is not
  insurmountable, it is computationally costly. However, for a valid energy score, a true distance
  is not necessary.  What is needed is a negative definite kernel.

\begin{prop}
For points $a,b \in \mathcal{S}_{\infty}^{d-1}$ a valid negative definite kernel can be formed as
  \begin{equation}
    g(\bm{a},\bm{b}) = \begin{cases}
        \pnorm{\bm{b}-\bm{a}}{2} &\text{ if }\argmax_{\ell}\bm{a} = \argmax_{\ell}\bm{b}\\
        \pnorm{\bm{c}-\bm{a}}{2} + \pnorm{\bm{b}-\bm{c}}{2} &\text{ otherwise}
    \end{cases}
  \end{equation}
  where $\bm{c}$ resides on the intersection between the faces of $\bm{a}$ and $\bm{b}$, and minimizes $g(\bm{a},\bm{b})$.
\end{prop}

In the first case, where $\bm{a}$ and $\bm{b}$ reside on the same face, then that is trivially a
  negative definite kernel.  In the second case, where $\bm{a}$ and $\bm{b}$ reside on separate
  faces, we can be assured of symmetry in the arguments as $\bm{c}$ is unique, and $\pnorm{\bm{c} - \bm{a}}{2} = \pnorm{\bm{a} - \bm{c}}{2}$.  Then for the other property of negative definiteness,
  \begin{equation*}
    \begin{aligned}
      \sum_{i = 1}^n\sum_{j = 1}^n \alpha_i\alpha_j g(\bm{x}_1,\bm{x}_2) &= \sum_{i = 1}^n\sum_{j = 1}^n \alpha_i\alpha_j \bigg[\pnorm{\bm{c} - \bm{x}_1}{2} + \pnorm{\bm{x}_2 - \bm{c}}{2}\bigg]\\
      &= \sum_{i = 1}^n\sum_{j = 1}^n \alpha_i\alpha_j\pnorm{\bm{c} - \bm{x}_1}{2} + \sum_{i = 1}^n\sum_{j = 1}^n \alpha_i\alpha_j\pnorm{\bm{x}_2 - \bm{c}}{2}
    \end{aligned}
  \end{equation*}
  which is less than $0$ in both terms by definition.

To compute this energy score as described would require optimization of the interim point $\bm{c}$.
  If we again consider the one-dimensional rotation in Equation~\ref{eq:1drotation}---we
  can view that as a numerically easier analogue accomplishing the same goal: the distance from
  the starting point, to some optimal point along the intersection between the starting and
  ending faces, to the ending point.  The strait line will pass through the optimal point, so
  the optimization of the interim point is accomplished \emph{en passant}.

% EOF
