\section{Scoring criteria for distributions in ${\mathbb S}_\infty^{d-1}$\label{sec:evaluation}}

In order to assess and compare the estimation of a distribution on ${\mathbb S}_\infty^{d-1}$ we consider 
    the theory of proper scoring rules developed in \cite{gneiting2007}. As mentioned in 
    Section~\ref{subsec:projgamma}, our approach does not provide a density on 
    ${\mathbb S}_{\infty}^{d-1}$, restricting our ability to construct model selection criteria 
    to sample-based approaches.  To this end, we employ the \emph{energy score} criterion introduced
    therein.
    % the We consider two scoring methods:  \emph{posterior predictive loss} (PPL) introduced in 
    % \cite{gelfand1998}, and \emph{energy score} (ES) introduced in \cite{gneiting2007}.

% \subsection{Posterior predictive loss}
% The posterior predictive loss criterion is introduced in \cite{gelfand1998}.  When we
%   assume a squared error loss function, then for the $i$th observation, the posterior predictive
%   loss criterion is computed as
%   \begin{equation}
%     \label{eq:ppl}
%     \begin{aligned}
%       &\hspace{-0.5cm}S_{\text{PPL}}^k\left(P,x_i\right) \\
%       &= \text{Var}_P\left[X_i\right] + \frac{k}{k+1}\left(\text{E}_P\left[X_{i}\right] - x_i\right)^2
%     \end{aligned}
%   \end{equation}
%   where $X_i$ is a random variable from the posterior predictive distribution for $x_i$.  The
%   scalar $k$ is a weighting factor that scales the importance of goodness of fit
%   relative to precision.  In fact a small $\text{Var}(X_{i})$ indicates high
%   precision, and a small $(\text{E}[X_{i}] - x_{i})^2$  indicates a good fit.  In our analysis,
%   we take the limit as $k\to\infty$, weighting both
%   equally. Overall, the smaller the score, the better.  Note that the PPLC score is defined 
%   for a univariate distribution.  As we are dealing
%   with a multivariate distribution, we will be taking the posterior predictive loss
%   summed over all dimensions.  That is,
%   \begin{equation}
%     \label{eq:ppl2}
%     \begin{aligned}
%       &\hspace{-0.5cm}S_{\text{PPL}}\left(P,\bm{x}_i\right) \\
%       &= \frac{1}{d}\sum_{\ell = 1}^d\left[\text{Var}_P\left[X_{i\ell}\right] + \left(\text{E}_P\left[X_{i\ell}\right] - x_{i\ell}\right)^2\right].
%     \end{aligned}
%   \end{equation}
%   We report the average $S_{\text{PPL}}$ taken from all observed data.

% \subsection{Energy score}
% We focus on \emph{energy scores} defined for a general probability distribution $P$, with finite expectation, as 
The energy score criterion defined for a general probability distribution $P$, with finite expectation, is developed
    as
    \begin{equation}
    \label{eq:es}
    \begin{aligned}
    &\hspace{-0.5cm}\text{S}_{\text{ES}}\left(P, \bm{x}_i\right) \\
    &=\text{E}_p \left[g\left(\bm{X}_i, \bm{x}_i\right)\right] -
                                    \frac{1}{2}\text{E}_p \left[g\left(\bm{X}_i,\bm{X}_i^{\prime}\right)\right],
    \end{aligned}
    \end{equation}
where $g$ is a kernel function. The score defined in Equation \eqref{eq:es} can be evaluated
  using samples from $P$, with the help of the law of large numbers.
  Moreover, Theorem~4 in \cite{gneiting2007}, states that if $g(\cdot,\cdot)$ is a negative 
  definite kernel, then $\text{S}(P,\bm{x})$ is a \emph{proper} scoring rule.  Recall that a 
  real valued function $g$ is a negative definite kernel if it is symmetric in its arguments, 
  and $\sum_{i=1}^n\sum_{j=1}^na_ia_jg(x_i,x_j)\leq 0$ for all positive integers $n$, and any 
  collection $a_1,\ldots,a_n\in{\mathbb R}$ such that  $\sum_{i = 1}^na_i = 0$.  

In a Euclidean space, these conditions are satisfied by the Euclidean distance \citep{berg1984}. 
    However, for observations on different faces of ${\mathbb S}_{\infty}^{d-1}$, Euclidean 
    distance will under-estimate the geodesic distance, the actual distance required to travel 
    between the two points.  Let
\begin{equation*}
    {\mathbb C}_{\ell}^{d-1} = \lbrace \bm{x} : \bm{x} \in {\mathbb S}_{\infty}^{d-1}, x_{\ell} = 1\rbrace
\end{equation*}
comprise the $\ell$th \emph{face}.  For points on the same face, Euclidean distance 
  corresponds to the length of the shortest possible path in ${\mathbb S}_{\infty}^{d-1}$.  
  For points on different faces, the Euclidean distance is a lower bound for that length.

For a finite $p$, the shortest connecting path between two points in ${\mathbb S}_p^{d-1}$
  is the minimum geodesic; its length satisfying the definition of a distance.  Thus its
  length can be used as a negative definite kernel for the purpose of defining an energy
  score. Unfortunately as $p\to\infty$ the resulting surface ${\mathbb S}_{\infty}^{d-1}$
  is not differentiable, implying that routines to calculate geodesics are not readily
  available.  However, as ${\mathbb S}_{\infty}^{d-1}$ is a portion of a $d$-cube, we
  can borrow a result from geometry \citep{pappas1989} stating that the length of the
  shortest path between two points on a geometric figure corresponds to the length of a
  straight line drawn between the points on an appropriate unfolding, rotation, or 
  \emph{net} of the figure from a $d$-dimensional to a $d-1$-dimensional space.  The optimal 
  net will have the shortest straight line between the points, as long as that line is fully 
  contained within such net. As ${\mathbb S}_{\infty}^{d-1}$ has $d$ faces---each face 
  pairwise adjacent, there are $d!$ possible nets.  However, we are only interested in nets 
  that begin and end on the source and destination faces respectively, reducing the number 
  of nets under consideration to $\sum_{k = 0}^{d-2}\binom{d-2}{k}$.  This is still 
  computationally burdensome for a large number of dimensions.  
  \makenote{However, we can efficiently establish an upper bound on the geodesic 
  length.  We use this upper bound on geodesic distance as the kernel function for
  the energy score.}

To calculate the energy score we define the kernel 
    \[  
    g(\bm{a},\bm{b}) = \min_{\bm{c} \in{\mathbb C}_{\jmath}^{d-1}\cap{\mathbb C}_{\ell}^{d-1}}\left\{ 
        \pnorm{\bm{c}-\bm{a}}{2} + \pnorm{\bm{b}-\bm{c}}{2} \right\}.
    \]
    where $\bm{a} \in {\mathbb C}_{\ell}^{d-1}$, and 
    $\bm{b} \in {\mathbb   C}_{\jmath}^{d-1}$, for $\ell,\jmath\in \{1,\ldots, d\}$.
%\end{prop}
%{\em Proof:}
%When $\bm{a}$ and $\bm{b}$ are on the same face, then $\ell = \jmath$, and $g$ is the Euclidean distance between $\bm{a}$ and $\bm{b}$, which is a  negative definite kernel.  When $\bm{a}$ and $\bm{b}$ are on separate
  %faces, let 
  %\[\bm{e} = 
    %\argmin_{\bm{c} \in{\mathbb C}_{\jmath}^{d-1}\cap{\mathbb C}_{\ell}^{d-1}}\left\{ 
        %\pnorm{\bm{c}-\bm{a}}{2} + \pnorm{\bm{b}-\bm{c}}{2} \right\}.
  %\]
  %We have that $g(\bm{a}, \bm{b}) = g(\bm{b}, \bm{a})$ due to the uniqueness of $\bm{e}$ and the symmetry of the Euclidean distance. Additionally, given $n$ and  a collection $\alpha_1,\ldots,\alpha_n\in{\mathbb R}$ such that  $\sum_{i = 1}^n\alpha_i = 0$, then
  %\begin{equation*}
    %\begin{aligned}
      %&\hspace{-0.5cm}\sum_{i = 1}^n\sum_{j = 1}^n \alpha_i\alpha_j g(\bm{x}_1,\bm{x}_2)\\
      %&= \sum_{i = 1}^n\sum_{j = 1}^n \alpha_i\alpha_j \bigg[\pnorm{\bm{e} - \bm{x}_1}{2} + \pnorm{\bm{x}_2 - \bm{e}}{2}\bigg]\\
      %&= \sum_{i = 1}^n\sum_{j = 1}^n \alpha_i\alpha_j\pnorm{\bm{e} - \bm{x}_1}{2}\\
      %&\hspace{1cm}+ \sum_{i = 1}^n\sum_{j = 1}^n \alpha_i\alpha_j\pnorm{\bm{x}_2 - \bm{e}}{2}\\
      %&\leq 0
    %\end{aligned}
  %\end{equation*}
  %as the Euclidean norm is a negative definite kernel.$\hfill\Box$
Evaluating $g$ as described requires the solution of a $(d-2)$-dimensional 
  optimization problem.  The following proposition provides a straightforward solution.
\begin{prop}\label{prop:rot}
    Let $\bm{a} \in {\mathbb C}_{\ell}^{d-1}$, and $\bm{b} \in {\mathbb C}_{\jmath}^{d-1}$, 
        for $\ell, \jmath \in \{1, \ldots , d\}$.  For $\ell\neq \jmath$, the 
  transformation $P_{\jmath\ell}(\cdot)$ required to rotate the $\jmath$th face along the 
  $\ell$th axis produces the vector $\bm{b}^\prime$, where
  \begin{equation}
    \label{eqn:rotation}
     \bm{b}^{\prime}_i = P_{\jmath\ell}(\bm{b}) = 
    \begin{cases}
        b_{i} &\text{for }i\neq \jmath,\ell\\
        1 &\text{for }i = \ell\\
        2 - b_{\ell} &\text{for }i = \jmath
    \end{cases}.
  \end{equation}
  Then $g(\bm{a},\bm{b}) = \pnorm{\bm{a} - \bm{b}^{\prime}}{2}$.
\end{prop}
{\em Proof:}
Notice that for 
  $\bm{c} \in {\mathbb C}_{\jmath}^{d-1}\cap{\mathbb C}_{\ell}^{d-1}$,
     $\pnorm{\bm{b} - \bm{c}}{2} =  \pnorm{\bm{b}^{\prime} - \bm{c}}{2}$ and let
\[\bm{e} = 
    \argmin_{\bm{c} \in{\mathbb C}_{\jmath}^{d-1}\cap{\mathbb C}_{\ell}^{d-1}}\left\{ 
        \pnorm{\bm{c}-\bm{a}}{2} + \pnorm{\bm{b}-\bm{c}}{2} \right\}.
\]
We then have that
  \begin{equation*}
    \begin{aligned}
    g(\bm{a}, \bm{b}) &= \pnorm{\bm{a} - \bm{e}}{2} + \pnorm{\bm{b} - \bm{e}}{2}\\
    &= \pnorm{\bm{a} - \bm{e}}{2} + \pnorm{\bm{b}^{\prime} 
        - \bm{e}}{2} = \pnorm{\bm{a} - \bm{b}^{\prime}}{2}.
    \end{aligned}
  \end{equation*}
  The last equality is due to the fact that $\bm{a}$ and $\bm{b}'$ belong to the 
  same hyperplane. $\hfill\Box$
    
Using the rotation in Proposition \ref{prop:rot} we obtain the following result.
    \begin{prop}\label{prop:g}
    g is a negative definite kernel.
    \end{prop}
    {\em Proof:} For a given $n$ consider an arbitrary set of points
    $\bm{a}_1, \ldots , \bm{a}_n \in {\mathbb S}_\infty^{d-1}$, and 
    $\alpha_1, \ldots , \alpha_n \in {\mathbb R}$, such that $\sum_{i=1}^n
    \alpha _i = 0$. Then
    \[
    \sum_{i,\jmath} \alpha_i \alpha_\jmath g(\bm{a}_i, \bm{a}_\jmath) 
        = \sum_{i,\jmath} \alpha_i \alpha_\jmath 
    \|\bm{a}_i - \bm{a}_\jmath^\prime\|_2 \leq 0,
    \]
    where $\bm{a}_\jmath^\prime$ is defined as in Proposition \ref{prop:rot}. $\hfill\Box$

Proposition \ref{prop:rot} provides a computational efficient way to evaluate the proper
    scoring rule $S_{\text{ES}}$ defined on ${\mathbb S}_\infty^{d-1}$, for each 
    observation. For the purpose of model assessment and comparison, we report the 
    average $S_{\text{ES}}$ taken across all observed data.  In this form, the 
    interpretation of $S_{\text{ES}}$ is similar to that of $S_{\text{PPL}}$: The smaller 
    the score, the better.
    
% EOF
