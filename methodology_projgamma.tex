\subsection{Projected gamma family}
  A natural  distribution to consider in ${\mathbb R}^p_+$ is given by a product of independent
  univariate Gamma distributions. Let
    $\bm{ X} \sim \prod_{\ell = 1}^d\text{Ga}\left(X_{\ell}\mid\alpha_{\ell},\beta_{\ell}\right)$, 
    $a_\ell$ and $b_\ell$ are the shape and scale parameters, repectively. Using Equations (\ref{eqn:pnormt}) -- (\ref{eqn:pnormjac}) we have the joint density
  \begin{equation*}
    f(r,\bm{ y}) = \prod_{\ell = 1}^{d}
      \left[\frac{\beta_{\ell}^{\alpha_{\ell}}}{\Gamma(\alpha_{\ell})}(ry_{\ell})^{\alpha_{\ell} - 1}
          \exp\lbrace-\beta_{\ell}ry_{\ell}\rbrace\right]
      \times r^{d-1}\left[y_d -
            {\textstyle \sum}_{\ell = 1}^{d-1}y_{\ell}^p\left(y_d^p\right)^{\frac{1}{p} - 1}\right],
  \end{equation*}
  Integrating out $r$ yields the resulting \emph{Projected Gamma} density
  \begin{equation}
    \label{eqn:projgamma}
    \text{PG}(\bm{ y}\mid\bm{ \alpha},\bm{ \beta}) =
          \prod_{\ell = 1}^d\left[\frac{\beta_{\ell}^{\alpha_{\ell}}}{\Gamma(\alpha_{\ell})}
                y_{\ell}^{\alpha_{\ell} - 1}\right]
      \times \left[y_d -
          {\textstyle \sum}_{\ell = 1}^{d-1}y_{\ell}^p\left(y_d^p\right)^{\frac{1}{p} - 1}\right]
      \times \frac{\Gamma({\textstyle\sum}_{\ell = 1}^d\alpha_{\ell})}{\left({\textstyle\sum}_{\ell = 1}^d
                    \beta_{\ell}y_{\ell}\right)^{{\scriptstyle\sum_{\ell = 1}^d \alpha_{\ell}}}}
  \end{equation}
  defined for $\bm{y}\in \mathcal{S}_p^{d-1}$, and for any $p>0$. To avoid idetifiability problems when estimating the shape and scale parameters, we set $\beta_1 = 1$.
  \cite{nunez2019} obtain the density in Equation (\ref{eqn:projgamma})
  for $p=2$ as a multivariate distribution for directional data, using spherical coordinates.
  For $\bm{ y}\in \mathcal{S}_1^{d-1}$, and $\beta_{\ell} = \beta, \forall\ell$, the density in Equation (\ref{eqn:projgamma}) corresponds to that of a Dirichlet distribution.
  
  The projected gamma family is simple to specify and has very tractable computational properties. Thus, we use it as a building block for the angular measure $\Phi$ models. To build a flexible family of distributions in $\mathcal{S}_p^{d-1}$ we consider mixtures of projected gamma densities defined as
  \begin{equation} \label{PGmix}     
     f(\bm{y}) = \int_\Theta \text{PG}(\bm{y}|\bm{\theta}) dG(\bm{\theta}), 
  \end{equation} 
  where $\bm{\theta} = (\bm{\alpha}, \bm{\beta})$. Following a Bayesian non-parametric approach \citep{Ferguson74,Antoniak1974} we assume that $G$ is drawn from a random measure. In particular, assuming a Dirichlet process prior, we have a hierarchical formulation of the mixture model given by
  \[    \bm{y} \sim \text{PG}(\bm{y}|\bm{\theta}) , \;\;\; \bm{\theta} \sim G, \;\;\; G \sim \text{DP}(\eta, G_0),\]
  where DP denotes a Dirichlet process, $\eta$ is the precision parameter, and $G_0$ is the centering distribution. 
  
  Our strategy consists of describing the angular distribution $\Phi$ using a sample based approach with the following steps: (i) Apply the transformation in Equation (\ref{eqn:standardization}) to the original data; (ii) Obtain the subsample of the standardized observations that satisfy $R>1$; (iii) Take a finite $p$ and project the observations onto $\mathcal{S}_p^{d-1}$; (iv) Fit the model in Equation (\ref{PGmix}) to the resulting data and obtain samples from the fitted model; (v) Project the resulting samples onto $\mathcal{S}_\infty^{d-1}$.
  For step (iv) we use a Bayesian approach that is implemented using a purposely developed Markov chain Monte Carlo that is described in the next section. 
  {\bf Instead of the olf Figure 1, it would be nice to add a figure where you illustrate this procedure. Give it a try.}
%  The full conditional for $r$ takes the form
%  \begin{equation}
%    \label{eqn:rfullcond}
%    r\mid\bm{ \alpha},\bm{ \beta}, y \sim \text{Ga}\left(r\mid{\textstyle\sum}_{\ell = 1}^d
%                    \alpha_{\ell}, {\textstyle\sum}_{\ell = 1}^d \beta_{\ell}y_{\ell}\right).
%  \end{equation}

\subsubsection{Inference for the projected gamma mixture model}
{\bf Our focus is on model (\ref{PGmix}). Describe how you build a sample based approach to fitting that model. How you introduce latent variables, both to break the mixture and to produce the PG. You can then discuss variations that essentially consists on different priors. So, basically, you need to condense the next three pages in about a page.}

We consider a basic model that assumes all observations are generated by a single projected Gamma
  distribution.  The shape and rate parameters for the latent \text{Gamma} random variables,
  $\alpha_{\ell},\beta_{\ell}$, must be greater than 0, so a logical prior to use would be a Gamma prior for
  each parameter, excepting for $\beta_1 := 1$.
  \begin{equation}
    \label{eqn:vanillap}
    \begin{aligned}
      \bm{ y}\mid\alpha,\beta &\sim \text{PG}(\bm{ y}\mid\alpha,\beta)\\
      \bm{ \alpha},\bm{\beta} &\sim {\textstyle \prod}_{\ell = 1}^d \text{Ga}(\alpha_{\ell} \mid \xi_{\ell},\tau_{\ell})
              \times {\textstyle \prod}_{\ell = 2}^d \text{Ga}(\beta_{\ell}\mid \zeta_{\ell},\sigma_{\ell}).
    \end{aligned}
  \end{equation}
  A useful property of the Projected Gamma family is the latent independent Gammas assumption.  Recall
  in Eqn.~\ref{eqn:protopg}, we have a density defined as a product of independent Gamma densities,
  multiplied by a factor of $r^{d-1}C$, where $C$ is constant with respect to the parameters
  $\alpha,\beta$, as well as $r$.  If we conduct inference via a Gibbs sampler, then by simulating
  this latent $r$ we are able to perform inference of $(\alpha_{\ell},\beta_{\ell})$ independent of
  that of $(\alpha_k,\beta_k)$.  The full conditional for $r$ is described in Eqn.~\ref{eqn:rfullcond}.
  The full conditional for $\beta_{\ell}$, $l = 2,\ldots,d$ then becomes
  \begin{equation}
    \label{eqn:betafc}
    \beta_{\ell}\mid \bm{ y}, r, \alpha_{\ell} \sim \text{Ga}\left(n + \alpha_{\ell} + \zeta_{\ell},
                                      {\textstyle \sum}_{i = 1}^nr_iy_{il} + \sigma_{\ell}\right).
  \end{equation}
  That is, given $\alpha_{\ell}$, $r$, we can sample $\beta_{\ell}$ directly as a Gamma random
  variable.  In the sampling for $\alpha_{\ell}$, we can integrate $\beta_{\ell}$ out leaving us a
  quantity proportional to a density, which we can sample via the Metropolis Hastings
  algorithm\citep{hastings1970}.  For $\alpha_{\ell}$, $\ell = 2,\ldots, d$, This takes the form
  \begin{equation}
    \label{eqn:alphafc}
    f(\alpha_{\ell} \mid \bm{ y}, r) \propto
    \frac{\left({\textstyle \prod}_{i = 1}^nr_iy_{il}\right)^{\alpha_{\ell} - 1}}{\Gamma^n(\alpha_{\ell})} \times
    \alpha_{\ell}^{\xi_{\ell} - 1}\exp\{-\tau_{\ell}\alpha_{\ell}\} \times
    \frac{\Gamma(n\alpha_{\ell} + \zeta_{\ell})}{
        \left({\textstyle\sum}_{i = 1}^n r_iy_{il} + \sigma_{\ell}\right)^{(n * \alpha_{\ell} + \zeta_{\ell})}}.
  \end{equation}
  For $\alpha_1$, as we do not integrate out $\beta_1$ as it is defined to be $1$, this takes the
  slightly simpler form
  \begin{equation}
    \label{eqn:alphafc1}
    f(\alpha_1 \mid \bm{ y}, r) \propto
      \frac{{\textstyle\prod}_{i = 1}^n ry_{i1}^{\alpha_1 - 1}}{\Gamma^n(\alpha_1)} \times
      \alpha_1^{\xi_1 - 1}\exp\{-\tau_1\alpha_1\}.
  \end{equation}
  The \emph{projected restricted Gamma} model sets $\beta_{\ell} := 1 \forall \ell \in \lbrace 1,\ldots,d\rbrace$,
  so in that model inference on all $\alpha_{\ell}$ will use the simpler form.

\subsubsection{Finite Mixture Model}
The projected Gamma distribution offers a flexible model for describing direction.  However, it is
  conceivable that a single Projected Gamma distribution will not sufficiently capture the
  variability of real data.  In that case, a logical extension to make to the above model is to
  assume the data, $\bm{ y}$, are generated from a mixture of projected Gammas.  We consider a finite
  mixture,
  \begin{equation}
    \label{eqn:fmixp}
    \begin{aligned}
      \bm{ y}_i &\sim \sum_{j = 1}^J\pi_j\text{PG}\left(\bm{ y}\mid \bm{ \alpha}_j, \bm{ \beta}_j\right)\\
      \bm{ \alpha}_j &\sim {\textstyle \prod}_{\ell = 1}^d \text{Ga}\left(\alpha_{\ell}\mid\xi_{\ell},\tau_{\ell}\right)\\
      \bm{ \beta}_j &\sim {\textstyle \prod}_{\ell = 2}^d \text{Ga}\left(\beta_{\ell}\mid\zeta_{\ell},\sigma_{\ell}\right)
    \end{aligned}
    \hspace{2cm}
    \begin{aligned}
      \bm{ \xi},\bm{\tau} &\sim {\textstyle \prod}_{\ell = 1}^d \text{Ga}(\xi_{\ell}\mid a,b) \times \text{Ga}(\tau_{\ell}\mid c,d)\\
      \bm{ \zeta},\bm{\sigma} &\sim {\textstyle\prod}_{\ell = 2}^d\text{Ga}(\zeta_{\ell} \mid a,b)\times \text{Ga}(\sigma_{\ell}\mid c,d)\\
      \bm{ \pi} &\sim \text{Dir}(\pi_0)
    \end{aligned}
  \end{equation}
  In addition to allowing multiple mixture components, we assume those mixture components themselves
  descend from a product of independent Gamma densities. The standard method for sampling from a
  mixture distribution includes a data augmentation step, generating a
  $\delta_i \in \lbrace 1,\ldots, J\rbrace$ indicating to which mixture component a
  particular observation comes from, with
  \begin{equation*}
    \text{P}\left[\delta_i = j\mid\alpha_j,\beta_j\right] \propto \pi_j\text{PG}\left(\bm{ y}_i\mid \bm{ \alpha}_j,\bm{ \beta}_j\right).
  \end{equation*}
  By sampling and conditioning on $\delta_i$, we end up with very similar semi-conjugate forms for
  inference of $\bm{ \alpha}_j$, $\bm{ \beta}_j$ that were used in the basic model.  The changes made
  to Equations~\ref{eqn:betafc},\ref{eqn:alphafc},\ref{eqn:alphafc1} are such that we're conducting inference
  on a mixture component given the mixture component identifiers $\bm{ \delta}$, so instead of indexing
  $i$ over the whole data set, we're indexing $i$ over the set $\{i :\delta_i = j\}$, with $n = n_j$,
  the cardinality of that set. Inference on $\bm{ \xi,\tau,\zeta,\sigma}$ follows a similar form as
  that of $\bm{ \alpha,\beta}$---they are Gamma-Gamma relationships, so rate parameters have a conjugate
  Gamma posterior, and we can integrate them out to perform inference on the shape parameters via
  Metropolis Hastings.

Additionally, we investigate a finite mixture extension of the \emph{Projected Restricted Gamma} model,
  where we define all $\beta_{\ell} := 1$, for $l = 1,\ldots,d$, for $j = 1,\ldots,J$.  This affects the above
  model by removing inference steps for $\bm{ \beta},\bm{ \zeta},\bm{ \sigma}$.  The inference for
  $\alpha_{\ell}$ for all $\ell = 1,\ldots,d$ follows the form in Equation~\ref{eqn:alphafc1}; inference on
  $\bm{ \xi,\tau}$ follows the same form as the unrestricted model.  The impetus for the restricted
  gamma model is that in the context of a mixture model, the additional flexibility offered by the
  unrestricted Gamma model may be better represented with additional clusters.

\subsubsection{Dirichlet Process Mixture Model}
A logical extension to the finite mixture model described in Equation~\ref{eqn:fmixp} would be to assume the
  $\bm{ y}$'s descending from an infinite mixture model.  This to some extent solves the issue of
  selecting the number of mixture components, as that will effectively be done so in the model.
  \begin{equation}
    \label{eqn:infmixp}
    \begin{aligned}
      \bm{ y}_i &\sim \sum_{j = 1}^J\pi_j\text{PG}\left(\bm{ y}\mid \bm{ \alpha}_i, \bm{ \beta}_i\right)\\
      (\alpha_i,\beta_i) &\sim \text{DP}\left(\eta, G_0\right)\\
      &~\hspace{-2cm}G_0 = {\textstyle\prod}_{\ell = 1}^d\text{Ga}\left(\alpha_{jl}\mid\xi_{l},\tau_{l}\right)
                    \times{\textstyle\prod}_{\ell = 2}^d\text{Ga}\left(\beta_{jl}\mid\zeta_{l}\tau_{l}\right)
    \end{aligned}
    \hspace{1cm}
    \begin{aligned}
      \bm{ \xi},\bm{ \tau} &\sim {\textstyle \prod}_{\ell = 1}^d \text{Ga}(\xi_{\ell}\mid a,b)\times \text{Ga}(\tau_{\ell}\mid c,d)\\
      \bm{ \zeta},\bm{\sigma} &\sim {\textstyle\prod}_{\ell = 2}^d\text{Ga}(\zeta_{\ell} \mid a,b)\times \text{Ga}(\sigma_{\ell}\mid c,d)\\
      \bm{ \eta} &\sim \text{Ga}(\eta \mid 2, 0.1).
    \end{aligned}
  \end{equation}
  To fit the Dirichlet process, we use a variant of the collapsed Gibbs sampler detailed as
  Algorithm~8 in \citet{neal2000}.  In this algorithm, we perform a data augmentation to create a
  cluster identifier, $\delta_i$, for each observation. Let $J$ indicate the number of extant clusters.
  Under the collapsed sampler scheme, for existing clusters, the probability of cluster membership is
  calculated as
  \begin{equation}
    \label{eqn:dpdelta}
    \text{P}\left(\delta_i = j\mid r, \bm{ y_i} \bm{ \alpha}, \bm{ \beta}\right) \propto
        \frac{n_j^{\not i}}{\sum_{k = 1}^{J} n_k + \eta}
        {\textstyle\prod}_{\ell = 1}^d\text{Ga}\left(r_iy_{il}\mid\alpha_{jl},\beta_{jl}\right).
  \end{equation}
  where $n_j^{\not i} = {\textstyle\sum}_{k\neq i}1_{k == j}$.  Effectively, if the observation under review is a
  singleton, its existing cluster is destroyed.  For new clusters, ostensibly we would integrate out
  cluster parameters to achieve a posterior predictive density; though doing so in a
  non-conjugate model may be difficult.  Instead, the impetus for using this specific DP fitting algorithm
  is it simulates the posterior predictive density by generating $m$ new candidate clusters from
  their respective priors given $\bm{ \xi,\tau,\zeta,\sigma}$.  Then the probability of $y_i$
  belonging to a particular candidate cluster is calculated as
  \begin{equation}
    \label{eqn:dpdeltanew}
    \text{P}\left[\delta_i = k\mid r, \bm{ \alpha}_k^{\prime}, \bm{ \beta}_k^{\prime}\right] \propto
        \frac{\eta / m}{\sum_{j = 1}^{J} n_j + \eta}
        {\textstyle\prod}_{\ell = 1}^d\text{Ga}\left(r_iy_{il}\mid\alpha_{kl}^{\prime},\beta_{kl}^{\prime}\right),
  \end{equation}
  where $(\cdot)^{\prime}$ indicates a value as belonging to a new candidate cluster, and $k$ indexes
  along candidate clusters.  If a new cluster is selected, then
  $(\bm{\alpha}_{J+1},\bm{\beta}_{J+1}) = (\bm{\alpha}_k^{\prime},\bm{\beta}_k^{\prime})$, and $J = J + 1$.

Inference upon $\bm{ \alpha}_j,\bm{ \beta}_j$ operates in the same manner as with the finite mixture
  model.  For $\bm{ \xi,\tau,\zeta,\sigma}$, we have $J$ cluster components, indicating $J$
  observations that again are under a gamma-gamma model.  The rate parameters have a previously
  described conjugate posterior, that we can integrate out to more directly sample the shape
  parameters.  As before, we will be looking at this model, and the restricted gamma form of this model, where
  $\beta_{\ell} := 1$ for $\ell = 1,\ldots,d$.  This removes inference steps for $\bm{ \zeta},\bm{ \sigma}$
  as well.

\subsubsection{A Log-Normal Prior for Shape Parameters}
A final extension we can make to this model is to allow for some structure, or dependence, in the prior
  for shape parameters within a cluster.  The impetus for this approach is from recognizing that the
  algorithm we are using for sampling the DP may not adequately represent the posterior density for
  a new cluster, as it is assuming sampling from an independent prior.  If we account for dependence
  between dimensions within the prior, then we may be able to more efficiently sample new clusters.
  As shape parameters must be greater than 0, it is logical to assume a log normal prior, as that
  allows us the familiar forms of a mean vector and covariance matrix for representing the source of
  cluster parameters. So,
  \begin{equation}
    \label{eqn:dpln}
    \begin{aligned}
      \bm{ y}_i &\sim \sum_{j = 1}^J\pi_j\text{PG}\left(\bm{ y}\mid \bm{ \alpha}_i, \bm{\beta}_i\right)\\
      (\bm{\alpha}_i, \bm{\beta}_i) &\sim \text{DP}\left(\eta, G_0\right)\\
        &~\hspace{-1cm}G_0 = \mathcal{N}\left(\log\bm{ \alpha}_{j}\mid\mu,\Sigma\right)\times
            {\textstyle\prod}_{\ell = 2}^d\text{Ga}\left(\beta_{j,\ell}\mid\zeta_{\ell},\sigma_{\ell}\right)
    \end{aligned}
    \hspace{1cm}
    \begin{aligned}
      \mu,\Sigma &\sim \mathcal{N}\left(\mu\mid\mu_0,\Sigma_0\right) \times \text{IG}\left(\nu,\Psi\right)\\
      \bm{ \zeta},\bm{\sigma} &\sim {\textstyle\prod}_{\ell = 2}^d\text{Ga}(\zeta_{\ell} \mid a,b) \times \text{Ga}(\sigma_{\ell} \mid c,d) \\
      \bm{ \eta} &\sim \text{Ga}(\eta \mid 2, 0.1).
    \end{aligned}
  \end{equation}
  We hope that this will allow us to better incorporate the relationship between dimensions in
  sampling new clusters, allowing for better mixing of the model.  The downside is one of computational
  complexity---this introduces a $d$-dimensional normal distribution into the model, requiring a
  $d$-dimensional matrix inversion step.  As matrix inversion is computationally costly, this limits
  the potential number of dimensions possible.  The posterior inference strategies for $\beta,\zeta,\sigma$
  are the same as for prior models and have been described.  For inference on $\alpha_j$ given
  $r, \mu,\Sigma$, we rely on a $d$-dimensional Metropolis Hastings step, where the log-posterior
  density is proportional to
  \begin{equation}
    \label{eqn:dplnlp}
    \begin{aligned}
    f(\bm{\alpha}_j\mid\bm{\delta},\bm{y},\mu,\Sigma,\zeta,\sigma) &\propto
    \frac{\left(\prod_{i:\delta_i = j}y_{i,\ell}\right)^{\alpha_{j,\ell} - 1}}{\Gamma(\alpha_{j,\ell})^{n_j}}
        \prod_{\ell = 2}^d\left[\frac{\left(\prod_{i:\delta_i = j}y_{i,\ell}\right)^{\alpha_{j,\ell} - 1}
        \Gamma(n_j\alpha_{j,\ell} + \zeta)}{\Gamma(\alpha_{j,\ell})^{n_j}\left(\sum_{i:\delta_i = j}y_{i,\ell}
        + \sigma\right)^{n_j\alpha_{j,\ell} + \zeta}}\right]\\
        &\hspace{1cm}\times
        \lvert \Sigma\rvert^{-\frac{1}{2}}\prod_{\ell = 1}^d\alpha_{j,\ell}^{-1}\exp\left\lbrace
          -\frac{1}{2}(\log\alpha_j - \mu)^T\Sigma^{-1}(\log\alpha_j - \mu)\right\rbrace.
    \end{aligned}
  \end{equation}
  Updating $\beta_j$ is accomplished with a conjugate Gibbs step, using the full conditional detailed
  in Equation~\ref{eqn:betafc}, again subsetting $i$ such that $\delta_i = j$.  Note that we offer this
  model for completeness---in practice, attempting to learn $\beta$ with $\alpha$ having a log-normal
  prior resulted in extremely poor model performance.  We thus used the restricted gamma likelihood,
  assuming $\beta_{j,\ell} = 1$ for all $j,\ell$.  This simplifies Equation~\ref{eqn:dplnlp} into
  \begin{equation}
    \label{eqn:dplnlpr}
    \begin{aligned}
    f(\bm{\alpha}_j\mid\bm{\delta},\bm{y},\mu,\Sigma) &\propto
    \prod_{\ell = 1}^d\left[\frac{\left(\prod_{i:\gamma_i = j}
        y_{i,\ell}\right)^{\alpha_{j,\ell} - 1}}{\alpha_{j,\ell}\Gamma(\alpha_{j,\ell})^{n_j}} \right]
        \times\lvert\Sigma\rvert^{-\frac{1}{2}}
      \exp\left\lbrace-\frac{1}{2}(\log\alpha_j - \mu)^T\Sigma^{-1}(\log\alpha_j - \mu)\right\rbrace.
    \end{aligned}
  \end{equation}
  This model is conjugate for $\mu,\Sigma$, allowing simple Gibbs step updating.  The full
    conditional distributions for $\mu$, $\Sigma$ take the form
  \begin{equation}
    \begin{aligned}
    \mu\mid\Sigma,\alpha &\sim \mathcal{N}\left((n^*\Sigma^{-1} + S^{-1})^{-1}(n^*\overline{\log\alpha}^T\Sigma^{-1}
      + \mu_0^TS^{-1}),(n^{*}\Sigma^{-1} + S^{-1})^{-1}\right)\\
    \Sigma\mid\mu,\alpha &\sim \mathcal{IW}\left(
        n^* + \nu, {\textstyle\sum}_{j = 1}^{n^*}(\log\alpha_j - \mu)(\log\alpha_j - \mu)^T + \Psi\right),
    \end{aligned}
  \end{equation}
  where $n^*$ specifies the number of extant clusters.  Removing $\beta$ additionally removes the
  inference steps for $\zeta,\sigma$.

\subsubsection{Choice of Norm}
In the extreme value case, we have defined our task on $\mathcal{S}_{\infty}^{d-1}$, the positive orthant
  of the hypersphere defined under the $\mathcal{L}_{\infty}$ norm.  We previously described the
  difficulty in establishing a density directly on this geometry.  However, we can build the projected
  gamma distribution on $\mathcal{S}_p^{d-1}$ for a large $p$.  We can then project samples from the
  posterior predictive distribution for a model built on $\mathcal{S}_p^{d-1}$ onto
  $\mathcal{S}_{\infty}^{d-1}$. As $p$ increases, the closer the manifold will be to
  $\mathcal{S}_{\infty}^{d-1}$, and thus the closer the projected distribution will be to an idealized
  projected Gamma built on $\mathcal{S}_{\infty}^{d-1}$.  In testing, we tried developing models on
  $S_p^{d-1}$ for varying levels of $p$, and found that higher $p$ produced higher performing models.
  Additionally, the difference in model performance became more apparent as dimensionality increased.
  We present results here for models developed on $\mathcal{S}_{10}^{d-1}$.

% EOF
