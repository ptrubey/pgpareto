\subsection{Projected Gamma Family}
As stated previously, one approach we might try in order to generate a distribution on
  $\mathcal{S}_{p}^{d-1}$ would be to take a distribution in $\mathcal{R}_{+}^d$, reparameterize
  it in terms of radial and angular components, and integrate out the radial component in order to
  effectively project the distribution onto $\mathcal{S}_{p}^{d-1}$.  A natural candidate distribution
  to accomplish this is the Gamma.  Let $\bm{ X} \sim \prod_{l = 1}^d\text{Ga}\left(X_l\mid\alpha_l,\beta_l\right)$.
  Then transform $\bm{ X}$ via the transformation in Eqns.~\ref{eqn:pnormt} into $R,\bm{ Y}$, where
  $R = \pnorm{\bm{ X}}{p}$ and $Y\in \mathcal{S}_{p}^{d-1}$.  Completing the transformation, we
  incorporate the determinant of the Jacobian from Eqn.~\ref{eqn:pnormjac}.  Thus we have
  \begin{equation}
    \label{eqn:protopg}
    f(r,\bm{ y}) = \prod_{l = 1}^{d}
      \left[\frac{\beta_l^{\alpha_l}}{\Gamma(\alpha_l)}(ry_l)^{\alpha_l - 1}\exp\lbrace-\beta_lry_l\rbrace\right]
      \times r^{d-1}\left[y_d - {\textstyle \sum}_{l = 1}^{d-1}y_l^p\left(y_d^p\right)^{\frac{1}{p} - 1}\right],
  \end{equation}
  where $y_d = \left(1 - \sum_{l=1}^{d-1}y_l^p\right)^{\frac{1}{p}}$. Integrating out $r$ yields the
  the resulting \emph{Projected Gamma} density
  \begin{equation}
    \label{eqn:projgamma}
    \text{PG}(\bm{ y}\mid\bm{ \alpha},\bm{ \beta}) = \prod_{l = 1}^d\left[\frac{\beta_l^{\alpha_l}}{\Gamma(\alpha_l)}y_l^{\alpha_l - 1}\right]
      \times \left[y_d - {\textstyle \sum}_{l = 1}^{d-1}y_l^p\left(y_d^p\right)^{\frac{1}{p} - 1}\right]
      \times \frac{\Gamma({\textstyle\sum}_{l = 1}^d\alpha_l)}{\left({\textstyle\sum}_{l = 1}^d \beta_ly_l\right)^{{\scriptstyle\sum_{l = 1}^d \alpha_l}}}
  \end{equation}
  established for an arbitrary $\mathcal{L}_p$ norm. This forms a valid density for all finite $p$.
  We can regard the Dirichlet distribution as a special case of this density, where
  $\bm{ y}\in \mathcal{S}_1^{d-1}$, and $\beta_l = \beta$ for all $l$.  The distribution described
  in \cite{nunez2019} is related to this distribution in that it is established on
  $\bm{ y}\in \mathcal{S}_2^{d-1}$, transformed to spherical coordinates via Eqn.~\ref{eqn:spherical}.
  The full conditional for $r$ takes the form
  \begin{equation}
    \label{eqn:rfullcond}
    r\mid\bm{ \alpha},\bm{ \beta}, y \sim \text{Ga}\left(r\mid{\textstyle\sum}_{l = 1}^d \alpha_l, {\textstyle\sum}_{l = 1}^d \beta_ly_l\right).
  \end{equation}

In the estimation of the parameters of this model, we can quickly arrive at a problem, in that
  without some linear restriction on the rate parameters $\beta_l$, the model is not identifiable.
  The restriction most often used in the case of the Generalized Dirichlet, as well as by
  \cite{nunez2019} is to set $\beta_1 := 1$.

\subsubsection{Inference on Projected Gamma Model}
We consider a basic model that assumes all observations are generated by a single projected Gamma
  distribution.  The shape and rate parameters for the latent \text{Gamma} random variables,
  $\alpha_l,\beta_l$, must be greater than 0, so a logical prior to use would be a Gamma prior for
  each parameter, excepting for $\beta_1 := 1$.
  \begin{equation}
    \label{eqn:vanillap}
    \begin{aligned}
      \bm{ y}\mid\alpha,\beta &\sim \text{PG}(\bm{ y}\mid\alpha,\beta)\\
      \bm{ \alpha},\bm{\beta} &\sim {\textstyle \prod}_{l = 1}^d \text{Ga}(\alpha_l \mid \xi_l,\tau_l)
              \times {\textstyle \prod}_{l = 2}^d \text{Ga}(\beta_l\mid \zeta_l,\sigma_l).
    \end{aligned}
  \end{equation}
  A useful feature of the Projected Gamma family is the latent independent Gammas assumption.  Recall
  in Eqn.~\ref{eqn:protopg}, we have a density defined as a product of independent Gamma densities,
  multiplied by a factor of $r^{d-1}$ and some value, constant with respect to the parameters
  $\alpha,\beta$.  If we conduct inference via a Gibbs sampler, then by simulating this latent $r$
  we are able to perform inference of $(\alpha_l,\beta_l)$ independent of that of $(\alpha_k,\beta_k)$.
  The full conditional for $r$ is described in Eqn.~\ref{eqn:rfullcond}.  The full conditional for
  $\beta_l$, $l = 2,\ldots,d$ then becomes
  \begin{equation}
    \label{eqn:betafc}
    \beta_l\mid \bm{ y}, r, \alpha_l \sim \text{Ga}\left(n + \alpha_l + \zeta_l,
                                      {\textstyle \sum}_{i = 1}^nr_iy_{il} + \sigma_l\right).
  \end{equation}
  That is, given $\alpha_l$, $r$, we can sample $\beta_l$ directly as a Gamma random variable.  In
  the sampling for $\alpha_l$, we can integrate $\beta_l$ out leaving us a quantity proportional
  to a density, which we can sample via the Metropolis Hastings algorithm\citep{hastings1970}.  For $\alpha_l$,
  $l = 2,\ldots, d$, This takes the form
  \begin{equation}
    \label{eqn:alphafc}
    f(\alpha_l \mid \bm{ y}, r) \propto
    \frac{\left({\textstyle \prod}_{i = 1}^nr_iy_{il}\right)^{\alpha_l - 1}}{\Gamma^n(\alpha_l)} \times
    \alpha_l^{\xi_l - 1}\exp\{-\tau_l\alpha_l\} \times
    \frac{\Gamma(n\alpha_l + \zeta_l)}{
        \left({\textstyle\sum}_{i = 1}^n r_iy_{il} + \sigma_l\right)^{(n * \alpha_l + \zeta_l)}}.
  \end{equation}
  For $\alpha_1$, as we do not integrate out $\beta_1$ as it is defined to be $1$, this takes the
  slightly simpler form
  \begin{equation}
    \label{eqn:alphafc1}
    f(\alpha_1 \mid \bm{ y}, r) \propto
      \frac{{\textstyle\prod}_{i = 1}^n ry_{i1}^{\alpha_1 - 1}}{\Gamma^n(\alpha_1)} \times
      \alpha_1^{\xi_1 - 1}\exp\{-\tau_1\alpha_1\}.
  \end{equation}
  The \emph{projected restricted Gamma} model sets $\beta_l := 1 \forall l \in \lbrace 1,\ldots,d\rbrace$,
  so in that model inference on all $\alpha_l$ will use the simpler form.

\subsubsection{Finite Mixture Model}
The projected Gamma distribution offers a flexible model for describing direction.  However, it is
  conceivable that a single Projected Gamma distribution will not sufficiently capture the
  variability of real data.  In that case, a logical extension to make to the above model is to
  assume the data, $\bm{ y}$, are generated from a mixture of projected Gammas.  We consider a finite
  mixture,
  \begin{equation}
    \label{eqn:fmixp}
    \begin{aligned}
      \bm{ y}_i &\sim \sum_{j = 1}^J\pi_j\text{PG}\left(\bm{ y}\mid \bm{ \alpha}_j, \bm{ \beta}_j\right)\\
      \bm{ \alpha}_j &\sim {\textstyle \prod}_{l = 1}^d \text{Ga}\left(\alpha_l\mid\xi_l,\tau_l\right)\\
      \bm{ \beta}_j &\sim {\textstyle \prod}_{l = 2}^d \text{Ga}\left(\beta_l\mid\zeta_l,\sigma_l\right)
    \end{aligned}
    \hspace{2cm}
    \begin{aligned}
      \bm{ \xi},\bm{\tau} &\sim {\textstyle \prod}_{l = 1}^d \text{Ga}(\xi_l\mid a,b) \times \text{Ga}(\tau_l\mid c,d)\\
      \bm{ \zeta},\bm{\sigma} &\sim {\textstyle\prod}_{l = 2}^d\text{Ga}(\zeta_l \mid a,b)\times \text{Ga}(\sigma_l\mid c,d)\\
      \bm{ \pi} &\sim \text{Dir}(\pi_0)
    \end{aligned}
  \end{equation}
  In addition to allowing multiple mixture components, we assume those mixture components themselves
  descend from a product of independent Gamma densities. The standard method for sampling from a
  mixture distribution includes a data augmentation step, generating a
  $\delta_i \in \lbrace 1,\ldots, J\rbrace$ indicating to which mixture component a
  particular observation comes from, with
  \begin{equation*}
    \text{P}\left[\delta_i = j\mid\alpha_j,\beta_j\right] \propto \pi_j\text{PG}\left(\bm{ y}_i\mid \bm{ \alpha}_j,\bm{ \beta}_j\right).
  \end{equation*}
  By sampling and conditioning on $\delta_i$, we end up with very similar semi-conjugate forms for
  inference of $\bm{ \alpha}_j$, $\bm{ \beta}_j$ that were used in the basic model.  The changes made
  to Equations~\ref{eqn:betafc},\ref{eqn:alphafc},\ref{eqn:alphafc1} are such that we're conducting inference
  on a mixture component given the mixture component identifiers $\bm{ \delta}$, so instead of indexing
  $i$ over the whole data set, we're indexing $i$ over the set $\{i :\delta_i = j\}$, with $n = n_j$,
  the cardinality of that set. Inference on $\bm{ \xi,\tau,\zeta,\sigma}$ follows a similar form as
  that of $\bm{ \alpha,\beta}$---they are Gamma-Gamma relationships, so rate parameters have a conjugate
  Gamma posterior, and we can integrate them out to perform inference on the shape parameters via
  Metropolis Hastings.

Additionally, we investigate a finite mixture extension of the \emph{Projected Restricted Gamma} model,
  where we define all $\beta_l := 1$, for $l = 1,\ldots,d$, for $j = 1,\ldots,J$.  This affects the above
  model by removing inference steps for $\bm{ \beta},\bm{ \zeta},\bm{ \sigma}$.  The inference for
  $\alpha_l$ for all $l = 1,\ldots,d$ follows the form in Equation~\ref{eqn:alphafc1}; inference on
  $\bm{ \xi,\tau}$ follows the same form as the unrestricted model.  The impetus for the restricted
  gamma model is that in the context of a mixture model, the additional flexibility offered by the unrestricted Gamma model may be better represented with additional clusters.

\subsubsection{Dirichlet Process Mixture Model}
A logical extension to the finite mixture model described in Equation~\ref{eqn:fmixp} would be to assume the
  $\bm{ y}$'s descending from an infinite mixture model.  This to some extent solves the issue of
  selecting the number of mixture components, as that will effectively be done so in the model.
  \begin{equation}
    \label{eqn:infmixp}
    \begin{aligned}
      \bm{ y}_i &\sim \sum_{j = 1}^J\pi_j\text{PG}\left(\bm{ y}\mid \bm{ \alpha}_i, \bm{ \beta}_i\right)\\
      (\alpha_i,\beta_i) &\sim \text{DP}\left(\eta, G_0\right)\\
      &~\hspace{-2cm}G_0 = {\textstyle\prod}_{l = 1}^d\text{Ga}\left(\alpha_{jl}\mid\xi_{l},\tau_{l}\right)
                    \times{\textstyle\prod}_{l=2}^d\text{Ga}\left(\beta_{jl}\mid\zeta_{l}\tau_{l}\right)
    \end{aligned}
    \hspace{1cm}
    \begin{aligned}
      \bm{ \xi},\bm{ \tau} &\sim {\textstyle \prod}_{l = 1}^d \text{Ga}(\xi_l\mid a,b)\times \text{Ga}(\tau_l\mid c,d)\\
      \bm{ \zeta},\bm{\sigma} &\sim {\textstyle\prod}_{l = 2}^d\text{Ga}(\zeta_l \mid a,b)\times \text{Ga}(\sigma_l\mid c,d)\\
      \bm{ \eta} &\sim \text{Ga}(\eta \mid 2, 0.1).
    \end{aligned}
  \end{equation}
  To fit the Dirichlet process, we use a variant of the collapsed Gibbs sampler detailed as Algorithm~8 in \citet{neal2000}.  In this algorithm, we perform a data augmentation to create a cluster identifier, $\delta_i$, for each observation. Let $J$ indicate the number of extant clusters.  Under the collapsed sampler scheme, for existing clusters, the probability of cluster membership is calculated as
  \begin{equation}
    \label{eqn:dpdelta}
    \text{P}\left(\delta_i = j\mid r, \bm{ y_i} \bm{ \alpha}, \bm{ \beta}\right) \propto
        \frac{n_j}{\sum_{k = 1}^{J} n_k + \eta}
        {\textstyle\prod}_{l = 1}^d\text{Ga}\left(r_iy_{il}\mid\alpha_{jl},\beta_{jl}\right).
  \end{equation}
  where $n_j = {\textstyle\sum}_{k\neq i}1_{k == j}$.  Effectively, if the observation under review is a
  singleton, its existing cluster is destroyed.  For new clusters, ostensibly we would integrate out
  cluster parameters to achieve a posterior predictive density; though doing so in a
  non-conjugate model may be difficult.  Instead, the impetus for using this specific DP fitting algorithm
  is it simulates the posterior predictive density by generating $m$ new candidate clusters from their respective priors given
  $\bm{ \xi,\tau,\zeta,\sigma}$.  Then the probability of $y_i$ belonging to a particular candidate
  cluster is calculated as
  \begin{equation}
    \label{eqn:dpdeltanew}
    \text{P}\left[\delta_i = k\mid r, \bm{ \alpha}_k^{\prime}, \bm{ \beta}_k^{\prime}\right] \propto
        \frac{\eta / m}{\sum_{j = 1}^{J} n_j + \eta}
        {\textstyle\prod}_{l = 1}^d\text{Ga}\left(r_iy_{il}\mid\alpha_{kl}^{\prime},\beta_{kl}^{\prime}\right),
  \end{equation}
  where $(\cdot)^{\prime}$ indicates a value as belonging to a new candidate cluster, and $k$ indexes along candidate clusters.  If a new cluster is selected, then $(\bm{\alpha}_{J+1},\bm{\beta}_{J+1}) = (\bm{\alpha}_k^{\prime},\bm{\beta}_k^{\prime})$, and $J = J + 1$.

Inference upon $\bm{ \alpha}_j,\bm{ \beta}_j$ operates in the same manner as with the finite mixture
  model.  For $\bm{ \xi,\tau,\zeta,\sigma}$, we have $J$ cluster components, indicating $J$
  observations that again are under a gamma-gamma model.  The rate parameters have a previously
  described conjugate posterior, that we can integrate out to more directly sample the shape
  parameters.  As before, we will be looking at this model, and the restricted gamma form of this model, where
  $\beta_l := 1$ for $l = 1,\ldots,d$.  This removes inference steps for $\bm{ \zeta},\bm{ \sigma}$
  as well.

\subsubsection{A Log-Normal Prior for Shape Parameters}
A final extension we can make to this model is to allow for some structure, or dependence, in the prior
  for shape parameters within a cluster.  The impetus for this approach is from recognizing that the
  algorithm we are using for sampling the DP may not adequately represent the posterior density for
  a new cluster, as it is assuming sampling from an independent prior.  If we account for dependence
  between dimensions within the prior, then we may be able to more efficiently sample new clusters.
  As shape parameters must be greater than 0, it is logical to assume a log normal prior, as that
  allows us the familiar forms of a mean vector and covariance matrix for representing the source of
  cluster parameters. So,
  \begin{equation}
    \label{eqn:dpln}
    \begin{aligned}
      \bm{ y}_i &\sim \sum_{j = 1}^J\pi_j\text{PG}\left(\bm{ y}\mid \bm{ \alpha}_i, \bm{\beta}_i\right)\\
      (\bm{\alpha}_i, \bm{\beta}_i) &\sim \text{DP}\left(\eta, G_0\right)\\
        &~\hspace{-1cm}G_0 = \mathcal{N}\left(\log\bm{ \alpha}_{j}\mid\mu,\Sigma\right)\times
            {\textstyle\prod}_{l = 2}^d\text{Ga}\left(\beta_{jl}\mid\zeta_l,\sigma_l\right)
    \end{aligned}
    \hspace{1cm}
    \begin{aligned}
      \mu,\Sigma &\sim \mathcal{N}\left(\mu\mid\mu_0,\Sigma_0\right) \times \text{IG}\left(\nu,\Psi\right)\\
      \bm{ \zeta},\bm{\sigma} &\sim {\textstyle\prod}_{l = 2}^d\text{Ga}(\zeta_l \mid a,b) \times \text{Ga}(\sigma_l \mid c,d) \\
      \bm{ \eta} &\sim \text{Ga}(\eta \mid 2, 0.1).
    \end{aligned}
  \end{equation}
  We hope that this will allow us to better incorporate the relationship between dimensions in
  sampling new clusters, allowing for better mixing of the model.  The downside is one of computational
  complexity---this introduces a $d$-dimensional normal distribution into the model, requiring a
  $d$-dimensional matrix inversion step.  As matrix inversion is computationally costly, this limits
  the potential number of dimensions possible.  The posterior inference strategies for $\beta,\zeta,\sigma$
  are the same as for prior models and have been described.  For inference on $\alpha_j$ given
  $r, \mu,\Sigma$, we rely on a $d$-dimensional Metropolis Hastings step, where the log-posterior
  density is proportional to
  \begin{equation}
    \label{eqn:dplnlp}
    \begin{aligned}
    f(\bm{\alpha}_j\mid\bm{\delta},\bm{y},\mu,\Sigma,\zeta,\sigma) &\propto
    \frac{\left(\prod_{i:\delta_i = j}y_{il}\right)^{\alpha_{j1} - 1}}{\Gamma(\alpha_{j1})^{n_j}}
        \prod_{l = 2}^d\left[\frac{\left(\prod_{i:\delta_i = j}y_{il}\right)^{\alpha_{jl} - 1}
        \Gamma(n_j\alpha_{jl} + \zeta)}{\Gamma(\alpha_{jl})^{n_j}\left(\sum_{i:\delta_i = j}y_{il}
        + \sigma\right)^{n_j\alpha_{jl} + \zeta}}\right]\\
        &\hspace{1cm}\times
        \lvert \Sigma\rvert^{-\frac{1}{2}}\prod_{l=1}^d\alpha_{jl}^{-1}\exp\left\lbrace
          -\frac{1}{2}(\log\alpha_j - \mu)^T\Sigma^{-1}(\log\alpha_j - \mu)\right\rbrace.
    \end{aligned}
  \end{equation}
  Updating $\beta_j$ is accomplished with a conjugate Gibbs step, using the full conditional detailed
  in Equation~\ref{eqn:betafc}, again subsetting $i$ such that $\delta_i = j$.  Note that we offer this
  model for completeness---in practice, attempting to learn $\beta$ with $\alpha$ having a log-normal
  prior resulted in extremely poor model performance.  We thus used the restricted gamma likelihood,
  assuming $\beta_{jl} = 1$ for all $j,l$.  This simplifies Equation~\ref{eqn:dplnlp} into
  \begin{equation}
    \label{eqn:dplnlpr}
    \begin{aligned}
    f(\bm{\alpha}_j\mid\bm{\delta},\bm{y},\mu,\Sigma) &\propto
    \prod_{l = 1}^d\left[\frac{\left(\prod_{i:\gamma_i = j}y_{il}\right)^{\alpha_{jl} - 1}}{\alpha_{jl}\Gamma(\alpha_{jl})^{n_j}} \right]\times\lvert\Sigma\rvert^{-\frac{1}{2}}
      \exp\left\lbrace-\frac{1}{2}(\log\alpha_j - \mu)^T\Sigma^{-1}(\log\alpha_j - \mu)\right\rbrace.
    \end{aligned}
  \end{equation}
  This model is conjugate for $\mu,\Sigma$, allowing simple Gibbs step updating.  The full
    conditional distributions for $\mu$, $\Sigma$ take the form
  \begin{equation}
    \begin{aligned}
    \mu\mid\Sigma,\alpha &\sim \mathcal{N}\left((n^*\Sigma^{-1} + S^{-1})^{-1}(n^*\overline{\log\alpha}^T\Sigma^{-1}
      + \mu_0^TS^{-1}),(n^{*}\Sigma^{-1} + S^{-1})^{-1}\right)\\
    \Sigma\mid\mu,\alpha &\sim \mathcal{IW}\left(
        n^* + \nu, {\textstyle\sum}_{j = 1}^{n^*}(\log\alpha_j - \mu)(\log\alpha_j - \mu)^T + \Psi\right),
    \end{aligned}
  \end{equation}
  where $n^*$ specifies the number of extant clusters.  Removing $\beta$ additionally removes the
  inference steps for $\zeta,\sigma$.
  %
  % \bruno{\bf I think that 3.2.2--3.2.4 are pretty much the same model with some elaborations.
  %               You should be able to present the whole story in a much more condensed fashion.}

\subsubsection{Choice of Norm}
In the extreme value case, we have defined our task on $\mathcal{S}_{\infty}^{d-1}$, the positive orthant
  of the hypersphere defined under the $\mathcal{L}_{\infty}$ norm.  We previously described the
  difficulty in establishing a density directly on this geometry.  However, we can build the projected
  gamma distribution on $\mathcal{S}_p^{d-1}$ for a large $p$.  We can then project samples from the
  posterior predictive distribution for a model built on $\mathcal{S}_p^{d-1}$ onto
  $\mathcal{S}_{\infty}^{d-1}$. As $p$ increases, the closer the manifold will be to $\mathcal{S}_{\infty}^{d-1}$, and thus the closer the projected distribution will be to an idealized
  projected Gamma built on $\mathcal{S}_{\infty}^{d-1}$.  In testing, we tried developing models on
  $S_p^{d-1}$ for varying levels of $p$, and found that higher $p$ produced higher performing models.
  Additionally, the difference in model performance became more apparent as dimensionality increased.
  We present results here for models developed on $\mathcal{S}_{10}^{d-1}$.

% EOF
